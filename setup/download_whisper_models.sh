#!/bin/bash
# Download Whisper models for Pi-Jarvis
# Phase 2: Speech-to-Text Models

set -e  # Exit on any error

echo "ğŸ“¥ Pi-Jarvis Phase 2: Downloading Whisper Models"
echo "================================================"

# Ensure we're in the right directory
if [ ! -d "models/whisper.cpp" ]; then
    echo "âŒ Error: Whisper.cpp not built yet. Run ./setup/build_whisper.sh first"
    exit 1
fi

cd models/whisper.cpp

# Download tiny model (fast, lower accuracy)
echo "ğŸ“¦ Downloading Whisper tiny model (~39MB)..."
if [ ! -f "models/ggml-tiny.bin" ]; then
    bash ./models/download-ggml-model.sh tiny
    echo "âœ… Tiny model downloaded"
else
    echo "â„¹ï¸  Tiny model already exists"
fi

# Download small model (better accuracy, slower)
echo "ğŸ“¦ Downloading Whisper small model (~244MB)..."
echo "This may take a few minutes..."
if [ ! -f "models/ggml-small.bin" ]; then
    bash ./models/download-ggml-model.sh small
    echo "âœ… Small model downloaded"
else
    echo "â„¹ï¸  Small model already exists"
fi

# List downloaded models
echo ""
echo "ğŸ“‹ Downloaded models:"
ls -lh models/ggml-*.bin

cd ../..

# Copy models to our main models directory for easy access
echo "ğŸ“‚ Copying models to main models directory..."
cp models/whisper.cpp/models/ggml-tiny.bin models/
cp models/whisper.cpp/models/ggml-small.bin models/
echo "âœ… Models ready in models/ directory"

echo ""
echo "ğŸ¯ Models download complete!"
echo ""
echo "ğŸ“Š Model comparison:"
echo "  â€¢ tiny:  ~39MB,  fast inference, lower accuracy"
echo "  â€¢ small: ~244MB, slower inference, better accuracy"
echo ""
echo "ğŸš€ Next: Test speech-to-text functionality"
echo "Run: python -m assistant.test_stt"